---
title: My first two years at Sanoma Media Finland
draft: true
tags:
  - Philosophy
  - Career
date: 2025-08-01
categories:
  - Software development
authors:
  - ksaaskil
---

# My first two years at Sanoma Media Finland

This is a recap of my first two years at Sanoma Media Finland. During these two years, I have got to know great colleagues, worked on very interesting challenges and learned a lot about efficient teamwork.

<!-- more -->

## Fall 2023: Joining the team

In the Spring of 2023, I had grown increasingly frustrated about the state of product development in Silo AI and was therefore open for new opportunities. I was contacted by a headhunter in May 2023, who told me that Sanoma Media Finland (SMF) was searching for a new lead developer in their news personalization team. I knew that SMF had good values that aligned with mine (they had, for example, partnered with Helsinki Pride in earlier years) and I felt that working for a media company would feel personally more meaningful to me than consulting. Therefore, I accepted to join the interviews.

I was interviewed three times, interviewers including my future manager Jyrki, personalization team lead Katriina, lead data scientist Max and two lead developers in SMF, Miro and Janne. I liked everyone I met and learned that SMF had both interesting challenges related to data & AI as well as high quality standards for professional software development. After the three interview rounds, I was not elected as the lead developer, but I was offered a position as a senior developer in the same personalization team. It was agreed that I would have lead-like responsibilities in everything related to data architecture, which suited me well.

I joined the personalization team in August 2023. At that time, the team was responsible for operating both news personalization in sites such as [hs.fi](https://hs.fi) and [is.fi](https://is.fi) as well as developing a new real-time analytics (RTA) dashboard for newsrooms. The team included three consultants, two of them working on RTA. One of the two consultants was developing the real-time data processing engine and the other was developing the frontend. There was a strict deadline for the project to be running in production by the end of September 2023, because the contract with the previous SaaS analytics dashboard product was ending at that time.

Unfortunately, when I joined in August, the RTA project was at the brink of a collapse. The backend, which was using [Apache Flink](https://flink.apache.org/) for real-time data processing, was crashing almost daily. As the newsrooms already relied on the dashboard, they were constantly pinging the on-call team about the incidents, who got increasingly frustrated about the quality issues in the project. One of these incidents escalated so badly that the consulting backend developer working on the project went for sick leave and never returned to the company.

To save the project, our team formed a task force to fix what was broken. We noticed very quickly that the data processing component was not nearly production-ready. It did not have, for example, any deployment pipelines â€“ instead, the developer manually built and copied JAR files to S3 to deploy new versions. There were no unit tests: all changes to the calculation job could only be verified by deploying the job to production and running the job under production load. There was no monitoring, no alarms, no monitoring dashboards of any kind. There were no Flink snapshots, meaning that when the computation graph was changed, Flink started processing data again starting from the last midnight. The job had serious performance issues, so this backfill could take hours.

The project was not in a maintainable state. It had completely missed the [DevOps philosophy](https://abseil.io/resources/swe-book/html/ch02.html): getting feedback as early as possible, testing as early as possible, and especially thinking about security and production environments as early as possible. We explained the situation to the stakeholders and asked for time to re-write the data processing component totally from scratch. They understood the situation and gave us the time we needed. We created a temporary analytics dashboard in Databricks for the newsrooms and pulled the broken RTA project back to the development phase.

We fixed the component step-by-step by making small pull requests that changed one thing at a time. This allowed us to understand the effect of every change on the system behaviour and roll back bad changes as soon as possible. We migrated every computation from [Flink SQL](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/overview/) to [DataStream API](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/overview/), because the latter supported [fault tolerance via state snapshots](https://nightlies.apache.org/flink/flink-docs-release-2.0/docs/learn-flink/fault_tolerance/) and thereby allowed us to update the job in production without triggering any re-computation. We created unit tests for every pipeline component using [`MiniClusterWithClientResource`](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/testing/#junit-rule-miniclusterwithclientresource) that allows running a local Flink cluster for automated testing. We created a CI/CD pipeline in AWS CodePipeline for automatically updating the Flink application in production. We created a custom CloudWatch dashboard that showed every operational metric that we cared about. We created CloudWatch alarms with notifications, allowing us to report on and fix issues before newsrooms would notice them. We fixed performance issues by eliminating [data skew with salting](https://eng.lyft.com/gotchas-of-stream-processing-data-skewness-cfba58eb45d4) and [optimizing serialization](https://flink.apache.org/2020/04/15/flink-serialization-tuning-vol.-1-choosing-your-serializer-if-you-can/). We had multiple meetings with AWS Flink experts who helped us validate our assumptions and guided us in our endeavour.

By December 2023, we had a completely re-written Flink computation job that was ready for production. There were still minor hiccups, but the number of production issues was close to zero. I gave a presentation in Sanoma Developer Community with the title "10 Mistakes We Made with Apache Flink", explaining everything we had learned about running a production-grade real-time analytics system in Flink. I also gave a presentation about the re-written RTA dashboard for the on-call ring and the developer community. In total, I created 204 pull requests in the project.

This rewrite was a great opportunity for our team to learn to work together. We meticulously documented every issue in Confluence and created a step-by-step plan to tackle issues one by one. We learned to communicate between each other and to write small, targeted pull requests that were easy to review. We gained the stakeholders' trust by giving realistic time estimations and by focusing on building a production-grade system from the start.

## Spring 2024: New team, new habits

By the end of 2023, it had became clear that developing and maintaining the RTA dashboard in the personalization team was happening at the cost of news personalization. We had not delivered a single new feature in news personalization, which had direct negative impact on our readers. It was therefore decided that a new team would be formed that would take over the analytics responsibilities from the personalization team. The new personalization team, in which I stayed, would be lead by the lead data scientist Max.

Our first major task in the "new" personalization team was to stabilize and re-write the "most read" service. This service is responsible for serving the list of most read articles to our news sites. The legacy implementation was done using Kinesis Analytics, which was deprecated, unstable and created production incidents every few weeks.

We started designing the new system by mapping the needs and listing all available architecture alternatives with their pros and cons. We created architecture tradeoff analyses and decision records for all choices that mattered. We compared, for example, Python and TypeScript for programming languages, Lambda and Flink for data processing, and DynamoDB, Aurora and Redis for data storage.

Once we had a plan on what to build, we split the development into multiple milestones. Each milestone was split into small tasks that were maintained in Jira or Confluence. We had learned from the RTA project that every component should be built together as a team instead of letting one developer build, say, the data processing while the other developer built the API. We had some challenges in splitting the development tasks between developers, mostly because some of the tasks had intricate dependencies to other tasks and some developers were used to working on "end-to-end" tasks. But after all, I think we were very successful in splitting the work. Every piece in the system was built by at least two developers.

As we were building a business-critical system, we kept the DevOps philosophy in mind and minimized the risk of future production issues at every step of the development. Every piece in the system was carefully unit-tested. We built an integration test that automatically verified that the system as a whole worked as intended. This task was simplified by the fact that our team owned the whole data processing pipeline from data collection to serving the data. We ran the integration test as part of the CI/CD pipeline and blocked deployments that failed the test.

We implemented a custom Datadog dashboard for the system and ensured that all metrics that mattered were logged. We implemented canary release for every component, ensuring that faulty deployments would be automatically rolled back. At the code level, we made sure that exception was thrown and the system crashed for every unexpected situation, which allowed us to notice implementation errors as soon as they happened and fix them immediately.

Once the system was becoming close to production-ready, we implemented [shadow launch](https://devops.com/what-is-a-shadow-deployment/) to stress-test the system under realistic production load without the risk of affecting our users. After successful shadow launch, we gradually moved traffic from the old system to the new system, continuously monitoring that everything worked as expected.

The new system was very robust. As of today, there has not been a single production issue. Later, we removed the manual approval step in the deployment pipeline, letting all changes committed to the main branch to automatically move to production (assuming that the tests pass, of course). Personally, I created approximately 150 pull requests in the repository.

In addition to revising the above-mentioned service, we improved our news personalization in many ways. We improved subscription rates by integrating the personalization service with the near-real-time sales data, letting us boost the visibility of articles that generated a lot of subscriptions. We improved our "MLOps" capabilities by logging all our scoring service calls to Firehose and S3, which gave us direct visibility to the inputs and outputs of our machine learning components and thereby helped us debug many production issues. We also implemented server-side A/B testing, giving us the flexibility to run controlled A/B tests in the new mobile apps.

During this period, we set up the ways of working for the new personalization team. We established biweekly retros, where we reflected on how we could improve as a team. We created "deployment Mondays", in which we would every week automatically create a ticket for deploying all our ten-ish services to production. Deploying service to production also covered merging all pending dependency updates and checking our monitoring dashboards for any uncaught errors or warnings. On every Monday, our services were evenly divided between the developers, giving every developer only a few services to update.

Through deployment Mondays, we also improved other aspects of maintaining our services. Together with the rest of our software engineering organization, we enabled automerge for dependency updates. This reduced the developer burden on deployment Mondays, because most dependency updates were automatically merged. We created integration tests for all our services, helping developers rely more on test automation in ensuring that the services were OK to deploy to production.

During Spring 2024, I gave two presentations to the developer community in our department. The first presentation showed how we used Datadog to monitor the new most-read service, including custom metrics, monitors, SLOs and dashboards. I also gave a presentation about event-driven architecture, focusing on AWS EventBridge.

In April, I started mentoring a trainee developer as part of the [Sanoma Media Trainee](https://www.sanoma.com/fi/news/2024/sanoma-media-trainee-2025-haku-auki/) program. The trainee period was very fruitful to both of us and I learned a lot about mentoring a junior developer. The trainee was offered a permanent position in the company at the end of 2024 and they're a valuable part of the team today.

## Summer 2024: New features and old updates

In the Summer of 2024, our team's focus was to improve existing systems. We implemented a new simple algorithm for controlling the amount of paywalled articles shown in the frontpage.

I spent most of the summer updating the "tagging service" that was used to automatically propose the relevant tags for new articles. Updating the service included again a big "DevOps" overhaul, where we added everything considered a best practice in modern software development: linters, formatting, type-checking, monitoring, and proper dependency management. By adding proper Python packaging, we were able to remove all the `sys.path` hacks littered around the codebase. We also improved error handling, migrating away from so-called [Pokemon error handling](https://softwareengineering.stackexchange.com/questions/319088/is-indiscriminately-catching-exceptions-pokemon-exception-handling-ever-accept) and instead raising exceptions and crashing the service for any unexpected exceptions. This enabled powerful automated monitoring for the system.

There were no tests of any kind in the tagging service. The external developer who built the system years ago had even boasted that they did not believe in unit tests. Therefore, refactoring and maintaining the system was a big pain. We added simple unit tests to all the critical functionality and ensured that these tests were run as part of the pull request checks and the deployment pipeline, which greatly improved the confidence in test automation and enabled refactoring the code as needed. We also added end-to-end tests in the deployment pipeline to verify that training new models worked as expected.

Once the tagging service was considered production-grade, I moved to spending the rest of the summer on understanding in depth how our recommendation system works under the hood. I refreshed my memory about Bayesian data analysis (I took the [BDA course](https://avehtari.github.io/BDA_course_Aalto/) during my doctoral studies), learned about [Pyro](https://pyro.ai/) and [NumPyro](https://num.pyro.ai/en/latest/index.html#introductory-tutorials) and wrote an in-depth document with math equations about how user interest is modelled in our recommendation service. I also studied different [evaluation metrics for recommendation systems](https://aman.ai/recsys/metrics/) and implemented new metrics based on [counterfactual evaluation](https://eugeneyan.com/writing/counterfactual-evaluation/), with little benefit. I also compared different sampling algorithms, such as the [Markov Chain Monte Carlo](https://docs.pyro.ai/en/dev/mcmc.html) algorithm used in production and [stochastic variational inference](https://num.pyro.ai/en/latest/svi.html), finding that the latter produced similar results faster and with less computational resources.

In our tech improvement weekly, I presented an introduction to Bayesian inference based on everything I had learned during the summer. This presentation was fun to make, but it was probably the worst presentation I have given during my time in Sanoma. There was too much obscure math for a community of software engineers and my "simple" examples did not manage to convey the big ideas behind Bayesian inference. I had more success in presenting software design, mostly based on my favorite programming book [The Pragmatic Programmer](https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/).

## Fall 2024 â€“ Spring 2025

After a summer spent on learning and improving the maintainability of existing services, we spent the rest of year building new features for our core personalization services. We implemented support for collecting data about audio events from our websites and apps, allowing us to use the data for personalization and analytics. We started building a new experimental ML model for predicting users' purchase propensities and using it for personalizing the content on the frontpage. We implemented support for more structured personalized frontpage by extending our personalization to personalizing articles per topic and A/B-tested these changes. I gave a tech improvement presentation on how Google measures engineering productivity and improved our GDPR data exporting pipelines by refactoring the Glue Spark code, writing unit tests and adding new fields in the data export.

In the Fall of 2024, our team also started prototyping a new recommendation algorithm based on graph neural networks (GNNs). In the Spring of 2025, we built the infrastructure necessary for serving these new GNN-based recommendations to our users. This involved a lot of ML and data engineering happening mostly in Databricks, storing user and article embeddings to a vector database and creating a web server for serving the predictions. I contributed mostly to making sure these new services were production-grade, by watching over the system and code design and improving the deployment pipelines and monitoring.

## Summer 2025

In the Spring of 2025, I had started to feel that my time in the personalization team was coming to an end. We had built a high-performing team with high standards for quality and where both developers and scientists thrived and had plenty of opportunities to grow both professionally and as a person. The team was routinely building and maintaining high-quality production services with the best practices from DevOps and MLOps. I felt that I had contributed to the team in the best way I could and learned what I could about the news personalization. I'm very grateful for the time I got to spend in the personalization team.

In March 2025, it was announced internally that the company was looking for a lead developer to lead technical development in a new "Newsroom AI" team. I jumped to the opportunity, talked with various stakeholders and was finally offered the position. We interviewed and hired two developers and developer trainees to the new team, and I joined the team full-time in May 2025.

Writing this in July 2025, I have been working in the team for almost three months. It has been great so far. Our new hires were very successful and we have managed to build a team of very capable and enthusiastic people. Everything I learned about teamwork and ways of working in the personalization team has been immensely valuable in setting up the new team as well. We have established our ways of working and we're delivering software at a fast pace. We work in close collaboration with the users and stakeholders and I have high confidence that we will build very valuable products for our journalists. I have found a position where I can contribute to building great products using a methodology that I believe in, closing the circle started in the Summer of 2023.
