---
title: My first two years at Sanoma Media Finland
draft: true
tags:
  - Philosophy
  - Career
date: 2025-08-01
categories:
  - Software development
authors:
  - ksaaskil
---

# My first two years at Sanoma Media Finland

This is a recap of my first two years at Sanoma Media Finland. During these two years, I have got to know great colleagues, worked on very interesting challenges and learned a lot about efficient teamwork.

<!-- more -->

## Fall 2023: Joining the team

In the Spring of 2023, I had grown increasingly frustrated about the state of product development in Silo AI and was therefore open for new opportunities. I was contacted by a headhunter in May 2023, who told me that Sanoma Media Finland (SMF) was searching for a new lead developer in their news personalization team. I knew that SMF had good values that aligned with mine (they had, for example, partnered with Helsinki Pride in earlier years) and I felt that working for a media company would feel personally more meaningful to me than consulting. Therefore, I accepted to join the interviews.

I was interviewed three times, interviewers including my future manager Jyrki, personalization team lead Katriina, lead data scientist Max and two lead developers in SMF, Miro and Janne. I liked everyone I met and learned that SMF had both interesting challenges related to data & AI as well as high quality standards for professional software development. After the three interview rounds, I was not elected as the lead developer, but I was offered a position as a senior developer in the same personalization team. It was agreed that I would have lead-like responsibilities in everything related to data architecture, which suited me well.

I joined the personalization team in August 2023. At that time, the team was responsible for operating both news personalization in sites such as [hs.fi](https://hs.fi) and [is.fi](https://is.fi) as well as developing a new real-time analytics (RTA) dashboard for newsrooms. The team included three consultants, two of them working on RTA. One of the two consultants was developing the real-time data processing engine and the other was developing the frontend. There was a strict deadline for the project to be running in production by the end of September 2023, because the contract with the previous SaaS analytics dashboard product was ending at that time.

Unfortunately, when I joined in August, the RTA project was at the brink of a collapse. The backend, which was using [Apache Flink](https://flink.apache.org/) for real-time data processing, was crashing almost daily. As the newsrooms already relied on the dashboard, they were constantly pinging the on-call team about the incidents, who got increasingly frustrated about the quality issues in the project. One of these incidents escalated so badly that the consulting backend developer working on the project went for sick leave and never returned to the company.

To save the project, our team formed a task force to fix what was broken. We noticed very quickly that the data processing component was not nearly production-ready. It did not have, for example, any deployment pipelines â€“ instead, the developer manually built and copied JAR files to S3 to deploy new versions. There were no unit tests: all changes to the calculation job could only be verified by deploying the job to production and running the job under production load. There was no monitoring, no alarms, no monitoring dashboards of any kind. There were no Flink snapshots, meaning that when the computation graph was changed, Flink started processing data again starting from the last midnight. The job had serious performance issues, so this backfill could take hours.

The project was not in a maintainable state. It had completely missed the [DevOps philosophy](https://abseil.io/resources/swe-book/html/ch02.html): getting feedback as early as possible, testing as early as possible, and especially thinking about security and production environments as early as possible. We explained the situation to the stakeholders and asked for time to re-write the data processing component totally from scratch. They understood the situation and gave us the time we needed. We created a temporary analytics dashboard in Databricks for the newsrooms and pulled the broken RTA project back to the development phase.

We fixed the component step-by-step by making small pull requests that changed one thing at a time. This allowed us to understand the effect of every change on the system behaviour and roll back bad changes as soon as possible. We migrated every computation from [Flink SQL](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/overview/) to [DataStream API](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/overview/), because the latter supported [fault tolerance via state snapshots](https://nightlies.apache.org/flink/flink-docs-release-2.0/docs/learn-flink/fault_tolerance/) and thereby allowed us to update the job in production without triggering any re-computation. We created unit tests for every pipeline component using [`MiniClusterWithClientResource`](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/testing/#junit-rule-miniclusterwithclientresource) that allows running a local Flink cluster for automated testing. We created a CI/CD pipeline in AWS CodePipeline for automatically updating the Flink application in production. We created a custom CloudWatch dashboard that showed every operational metric that we cared about. We created CloudWatch alarms with notifications, allowing us to report on and fix issues before newsrooms would notice them. We fixed performance issues by eliminating [data skew with salting](https://eng.lyft.com/gotchas-of-stream-processing-data-skewness-cfba58eb45d4) and [optimizing serialization](https://flink.apache.org/2020/04/15/flink-serialization-tuning-vol.-1-choosing-your-serializer-if-you-can/). We had multiple meetings with AWS Flink experts who helped us validate our assumptions and guided us in our endeavour.

By December 2023, we had a completely re-written Flink computation job that was ready for production. There were still minor hiccups, but the number of production issues was close to zero. I gave a presentation in Sanoma Developer Community with the title "10 Mistakes We Made with Apache Flink", explaining everything we had learned about running a production-grade real-time analytics system in Flink. I also gave a presentation about the re-written RTA dashboard for the on-call ring and the developer community. In total, I created 204 pull requests in the project.

This rewrite was a great opportunity for our team to learn to work together. We meticulously documented every issue in Confluence and created a step-by-step plan to tackle issues one by one. We learned to communicate between each other and to write small, targeted pull requests that were easy to review. We gained the stakeholders' trust by giving realistic time estimations and by focusing on building a production-grade system from the start.

## Spring 2024: New team, new habits

By the end of 2023, it had became clear that developing and maintaining the RTA dashboard in the personalization team was happening at the cost of news personalization. We had not delivered a single new feature in news personalization, which had direct negative impact on our readers. It was therefore decided that a new team would be formed that would take over the analytics responsibilities from the personalization team. The new personalization team, in which I stayed, would be lead by the lead data scientist Max.

Our first major task in the "new" personalization team was to stabilize and re-write the "most read" service. This service is responsible for serving the list of most read articles to our news sites. The legacy implementation was done using Kinesis Analytics, which was deprecated, unstable and created production incidents every few weeks.

We started designing the new system by mapping the needs and listing all available architecture alternatives with their pros and cons. We created architecture tradeoff analyses and decision records for all choices that mattered. We compared, for example, Python and TypeScript for programming languages, Lambda and Flink for data processing, and DynamoDB, Aurora and Redis for data storage.

Once we had a plan on what to build, we split the development into multiple milestones. Each milestone was split into small tasks that were maintained in Jira or Confluence. We had learned from the RTA project that every component should be built together as a team instead of letting one developer build, say, the data processing while the other developer built the API. We had some challenges in splitting the development tasks between developers, mostly because some of the tasks had intricate dependencies to other tasks and some developers were used to working on "end-to-end" tasks. But after all, I think we were very successful in splitting the work. Every piece in the system was built by at least two developers.

As we were building a business-critical system, we kept the DevOps philosophy in mind and minimized the risk of future production issues at every step of the development. Every piece in the system was carefully unit-tested. We built an integration test that automatically verified that the system as a whole worked as intended. This task was simplified by the fact that our team owned the whole data processing pipeline from data collection to serving the data. We ran the integration test as part of the CI/CD pipeline and blocked deployments that failed the test.

We implemented a custom Datadog dashboard for the system and ensured that all metrics that mattered were logged. We implemented canary release for every component, ensuring that faulty deployments would be automatically rolled back. At the code level, we made sure that exception was thrown and the system crashed for every unexpected situation, which allowed us to notice implementation errors as soon as they happened and fix them immediately.

Once the system was becoming close to production-ready, we implemented [shadow launch](https://devops.com/what-is-a-shadow-deployment/) to stress-test the system under realistic production load without the risk of affecting our users. After successful shadow launch, we gradually moved traffic from the old system to the new system, continuously monitoring that everything worked as expected.

The new system was very robust. As of today, there has not been a single production issue. Later, we removed the manual approval step in the deployment pipeline, letting all changes committed to the main branch to automatically move to production (assuming that the tests pass, of course). Personally, I created approximately 150 pull requests in the repository.

In addition to revising the above-mentioned service, we improved our news personalization in many ways. We improved subscription rates by integrating the personalization service with the near-real-time sales data, letting us boost the visibility of articles that generated a lot of subscriptions. We improved our "MLOps" capabilities by logging all our scoring service calls to Firehose and S3, which gave us direct visibility to the inputs and outputs of our machine learning components and thereby helped us debug many production issues. We also implemented server-side A/B testing, giving us the flexibility to run controlled A/B tests in the new mobile apps.

During this period, we set up the ways of working for the new personalization team. We established biweekly retros, where we reflected on how we could improve as a team. We created "deployment Mondays", in which we would every week automatically create a ticket for deploying all our ten-ish services to production. Deploying service to production also covered merging all pending dependency updates and checking our monitoring dashboards for any uncaught errors or warnings. On every Monday, our services were evenly divided between the developers, giving every developer only a few services to update.

Through deployment Mondays, we also improved other aspects of maintaining our services. Together with the rest of our software engineering organization, we enabled automerge for dependency updates. This reduced the developer burden on deployment Mondays, because most dependency updates were automatically merged. We created integration tests for all our services, helping developers rely more on test automation in ensuring that the services were OK to deploy to production.

During Spring 2024, I gave two presentations to the developer community in our department. The first presentation showed how we used Datadog to monitor the new most-read service, including custom metrics, monitors, SLOs and dashboards. I also gave a presentation about event-driven architecture, focusing on AWS EventBridge.

In April, I started mentoring a trainee developer as part of the [Sanoma Media Trainee](https://www.sanoma.com/fi/news/2024/sanoma-media-trainee-2025-haku-auki/) program. The trainee period was very fruitful to both of us and I learned a lot about mentoring a junior developer. The trainee was offered a permanent position as a junior developer in the company at the end of 2024 and they're a valuable part of the team today.

## Summer 2024: Learning new, updating the old

In the Summer of 2024, we focused on improving existing systems. We implemented a new simple algorithm for controlling the amount of paywalled articles shown in the frontpage.

I spent most of the summer updating the "tagging service" that is used to automatically propose tags for new articles. Like the RTA service, also this service required a full "DevOps transformation", adding everything considered a best practice in modern software development: linters, formatting, type-checking, monitoring, and proper dependency management. By adding proper Python packaging, we were able to remove all `sys.path.append` hacks littered around the codebase. We also improved error handling, migrating away from so-called [Pokemon error handling](https://softwareengineering.stackexchange.com/questions/319088/is-indiscriminately-catching-exceptions-pokemon-exception-handling-ever-accept) and instead raising exceptions and crashing the service for any unexpected exceptions. This enabled powerful automated monitoring for the system and canary deployments with automated rollbacks.

There were no tests of any kind in the tagging service. The external developer who built the system years ago had even boasted in a developer community meeting that they did not believe in unit tests. Therefore, refactoring the system was a huge pain. We added simple unit tests to cover all the business-critical functionality and ensured that these tests were run as part of both the pull request checks and the deployment pipeline. This greatly improved our confidence in test automation and enabled refactoring the code more and more. We also added end-to-end tests in the deployment pipeline to verify that training new models and serving their predictions worked as expected.

Apart from updating the tagging service, I spent the Summer on studying in depth how our recommendation system works under the hood. I refreshed my memory about Bayesian data analysis (I took the [BDA course](https://avehtari.github.io/BDA_course_Aalto/) during my doctoral studies), learned about [Pyro](https://pyro.ai/) and [NumPyro](https://num.pyro.ai/en/latest/index.html#introductory-tutorials) and wrote an in-depth mathematical presentation about how users' interest in articles is modelled in our recommendation service. I also studied [evaluation metrics for recommendation systems](https://aman.ai/recsys/metrics/) and implemented new metrics based on [counterfactual evaluation](https://eugeneyan.com/writing/counterfactual-evaluation/), with little success. I also compared the performance of two different sampling algorithms, the [Markov Chain Monte Carlo](https://docs.pyro.ai/en/dev/mcmc.html) algorithm used in production and [stochastic variational inference](https://num.pyro.ai/en/latest/svi.html). I found that the latter produced equivalent results much faster and with less computational resources.

In our tech improvement weekly, I presented an introduction to Bayesian inference based on everything I had learned during the summer. This presentation was fun to make, but it was probably the worst presentation I have given during my time in Sanoma. There was too much obscure math for a community of software engineers and my examples failed to convey the big ideas behind Bayesian inference. I had more success in presenting software design, the presentation mostly being based on my favorite programming book [_The Pragmatic Programmer_](https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/).

## Fall 2024 â€“ Spring 2025: New features

After a summer spent on learning and improving the maintainability of existing services, we spent the rest of year building new features for our core personalization services. We implemented support for collecting audio event data from our websites and apps, allowing us to use this data for personalization and analytics. We built a new experimental machine learning model for predicting users' purchase propensities and tried using the model for personalizing the content on our frontpages.

We also implemented support for more structured personalized frontpage by extending our personalization to personalizing articles per topic. We A/B-tested these changes and decided to scrap the project for a while. I gave a tech improvement presentation on how [Google measures engineering productivity](https://abseil.io/resources/swe-book/html/ch07.html) and improved our data export pipelines by refactoring Glue Spark code, adding unit tests and adding support for audio events.

In the Fall of 2024, our team also started prototyping a new recommendation algorithm based on graph neural networks (GNNs). In the Spring of 2025, we built the infrastructure necessary for serving these new GNN-based recommendations to our users. This involved a lot of ML and data engineering happening mostly in Databricks, storing user and article embeddings to a vector database and creating a web server for serving the predictions. I contributed mostly to making sure these new services were production-grade, by contributing to the system and code design and improving the deployment pipelines and monitoring.

## Summer 2025: Promotion to lead and a new team

In the Spring of 2025, I had started to feel that my time in the personalization team was nearing its end. For more than a year, we had improved our ways of working to build a high-performing team with high standards for quality. I enjoyed working in the team and I was proud that all team members had so many opportunities to grow both professionally and as a person in the team. It had become a routine to us to build and maintain production services with the best practices from DevOps and MLOps. I was very grateful for the time I got to spend in the personalization team, but I felt that I could contribute more in some less mature team.

In March, it was announced internally that the company was looking for hire a lead developer to lead the technical development in a new "newsroom AI" team. I jumped to the opportunity, chatted with various stakeholders and was offered the position. To build the team, I and my colleagues interviewed and hired two developers and developer trainees to join the new team. I joined the team full-time in May 2025.

Writing this in July 2025, I have been working in the new team for almost three months. It has been great so far. Our hiring was very successful and we have managed to build a team of capable and enthusiastic people. Everything I learned about teamwork and setting up ways of working in the personalization team has been immensely valuable in setting up the new team as well. We have established solid ways of working and we're delivering software at a steady and predictable pace. I have a good relationship with our team's product owner and I believe our skill sets balance each other very well. We work in close collaboration with our users and I have high confidence that we will build very valuable products for our journalists in future. I believe I have now found a position where I can contribute to building great products using a methodology that I believe in. This closes the circle started in the beginning of this post.
